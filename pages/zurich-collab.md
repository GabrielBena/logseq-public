## First Meeting
	- ### Question 1: Are structurally modular networks better at solving compositionnal tasks ?
		- Is SM a good *inductive architectural bias* ?
		- At *equal number of parameters*, can modular networks outperfom monolithic ones ?
		- Can we take into account not only pure performance but also *sample efficiency* ?
			- Does this inductive bias allows for faster convergence towards good solutions?
			- Does it lead to faster fine-tuning / adaptation
				- This starts to go in the continual learning domain
		- Available literature:
			- [[@Neural Modularity Helps Organisms Evolve to Learn New Skills without Forgetting Old Skills]]
				- Oldie but goodie paper showing how evolved networks (no GD) end up being more modular when cost-minimization is applied, which in turn make them better at retaining knowledge
			- [[@Modular Growth of Hierarchical Networks: Efficient, General, and Robust Curriculum Learning]]
				- Paper showing that a (very) strong architectural bias perfectly suited for a task enables it to learn more tasks without forgetting, compared to monolothic arch trained end-to-end that suffers from forgetting and interferences.
			- [[@Is a Modular Architecture Enough?]]
				- Paper creating a family of task composed of rules to be applied to inputs, and measures the resulting modularity of trained networks and the effect of such modularity on generalization.
				- ((6703fd20-f78a-4c73-962f-ed385ca5e2bf))
			- [[@Combining Modular Skills in Multitask Learning]]
				- Paper showing modular design can lead to better sample efficiency and generalization in RL tasks.
			- [[@RECURRENT INDEPENDENT MECHANISMS]] /
			- [[@Systematic Generalization: What Is Required and Can It Be Learned?]]
			- [[@Modular Networks: Learning to Decompose Neural Computation]]
			- [[@Inductive biases of neural network modularity in spatial navigation]]
			-
	- ### Question 2 : For a given problem, what's the optimal modularity we need ?
		-