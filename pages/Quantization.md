- The goal here is to present a short overview of different papers regarding quantization in both #SNN and #ANN. Ideally, that'd lead to a more structured synthesis down the line.
- {{renderer :tocgen2}}
- # Literature Review:
	- ## [[@Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference]] :
	  collapsed:: true
		- ### Main Findings / Summary :
			- Quantization Scheme (affine mapping)
			- Quantization framework  for training (simulated quantization)
			- Quant-aware training helps to keep accuracies close to original, and contribute in advancing the latency-acc tradeoff.
	- ## [[@Quantizing deep convolutional networks for efficient inference: A whitepaper]]:
	  collapsed:: true
		- ### Main Findings / Summary:
			- Quantizing weights and activations to 8-bits post-training can produce classification accuracies within 2% of floating point networks for a wide variety of CNN architectures.
			- Model sizes can be reduced by a factor of 4 by quantizing weights to 8 bits, even when 8-bit arithmetic is not supported.
			- Quantization-aware training can further reduce the gap to floating point to 1% at 8-bit precision and allows for reducing the precision of weights to four bits with accuracy losses ranging from 2% to 10%.
		- ### Limitations:
			- The accuracy losses ranging from 2% to 10% with higher accuracy drop for smaller networks when reducing the precision of weights to four bits.
			- The need for further research to enhance the automated quantization tool to enable better quantization of networks.|
	- ## [[@Quantizing Spiking Neural Networks with Integers]]:
	  collapsed:: true
		- ### Main Findings / Summary:
			- Developing low-power SNNs is crucial for autonomous agents: SNNs trained with reduced precision can significantly reduce memory usage by ~73% with minimal test error increase.
			- The biggest effect comes from quantizing neural dynamics, while gradients and error propagation had marginal effects.
		- ### Limitations:
			- Need for further study on the interaction and trade-offs in the context of SNNs
			- The active areas of investigation regarding the effects of restricting the temporal dynamics in SNNs to forms that are amenable to efficient-digital hardware
			- The requirement for further investigations to better understand the complex trade-offs
	- ## [[@Hessian Aware Quantization of Spiking Neural Networks]]:
	  collapsed:: true
		- ### Main Findings / Summary :
			- The use of layer-wise Hessian trace analysis to guide the allocation of layer-specific bit-precision in quantizing a Spiking Neural Network (SNN).
			- Higher Hessian trace means bigger eigenvalues, aka less stability of the weights regarding gradient changes.
			- Hessian trace is tractable to compute using a random sample algorithm.
			- The Hessian trace increased 4-fold from layer L1 to L3, providing valuable information for determining the optimal layer-wise bit-precision for quantization.
				- Seems like deeper layers need more precision
			- The accuracy of the optimal quantized network only dropped by 0.3%, while the network size was reduced by 58%.
			- ((65d48090-91ce-4113-b5e8-c95d59130dc9))
			-
		- ### Limitations :
			- Small number of layers (3) but limitation of SNNs more than anything
			- Bit precision attribution performed by hand rather than using Pareto optimality front
			- Only simulated quantization
	- ## [[@Q-SpiNN: A Framework for Quantizing Spiking Neural Networks]]
	  collapsed:: true
		- ### Main Findings / Summary :
			- Core idea is to try and find the optimal mixed approach for quantizing not only weights, but other quantities as well (neuronal dynamics).
			- Indeed, different combination in precision could lead to the same accuracy, but with different memory footprints
				- Pareto Front
				- ((65d48102-013e-4516-ad73-dfe37cee4de0))
			- Could be combined nicely with Hessian analysis ?
			- An SNN family is created by choosing different schemes (Post training vs In training) and precisions per parameters. Then the pareto optimal family is extracted.
			- Results :
				- Unsupervised, able to keep acc in 1% and reduce footprint by ~4
				- Supervised : able to keep acc in 2% and reduce footprint by ~2
			- ((65d48126-84a3-431e-85cb-bbf64b22134d))
		- ### Limitations :
			- Seems very expensive ?
			- Results are not that impressive ?
			-
	- ## [[@Navigating Local Minima in Quantized Spiking Neural Networks]]:
	  collapsed:: true
		- ### Main Findings / Summary :
			- The paper presents a systematic evaluation of a cosine-annealed LR schedule coupled with weight-independent adaptive moment estimation for Quantized SNNs.
			- The use of cosine annealing scheduling can improve the performance of Spiking Neural Networks (SNNs) and Quantized SNNs (QSNNs) by escaping local minima and challenging networks with ill-defined gradients.
			- Performance degradation when quantizing weights is less with cosine annealing compared to other non-periodic schedules.
			- Cosine scheduling almost always achieved smaller variance of accuracy when compared to the alternatives, indicating its consistency in improving performance.
			- ((65d48153-97bb-4915-aeee-11b30580b920))
		- ### Limitations :
			- Existing mitigation techniques require additional memory or increased computational complexity during training
			- Avoidance of modification of neuron models or any other technique that is not common practice in DL to avoid adding overhead at runtime
			- Extended training duration for cosine annealing as a cost to consider
			- Need for more optimal step sizes and frequencies to improve results obtained from alternative schedules in the quantized case
	- ## [[@Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function]]:
	-