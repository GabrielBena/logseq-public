tags:: [[Computer Science - Artificial Intelligence]], [[Computer Science - Machine Learning]], [[Statistics - Machine Learning]]
date:: [[Nov 13th, 2018]]
extra:: arXiv: 1811.05249
title:: @Modular Networks: Learning to Decompose Neural Computation
item-type:: [[journalArticle]]
access-date:: 2021-10-21T09:38:32Z
original-title:: Modular Networks: Learning to Decompose Neural Computation
url:: http://arxiv.org/abs/1811.05249
short-title:: Modular Networks
publication-title:: "arXiv:1811.05249 [cs, stat]"
authors:: [[Louis Kirsch]], [[Julius Kunze]], [[David Barber]]
library-catalog:: arXiv.org
links:: [Local library](zotero://select/library/items/3EJRRR6S), [Web library](https://www.zotero.org/users/8224007/items/3EJRRR6S)

- [[Abstract]]
	- Scaling model capacity has been vital in the success of deep learning. For a typical network, necessary compute resources and training time grow dramatically with model size. Conditional computation is a promising way to increase the number of parameters with a relatively small increase in resources. We propose a training algorithm that flexibly chooses neural modules based on the data to be processed. Both the decomposition and modules are learned end-to-end. In contrast to existing approaches, training does not rely on regularization to enforce diversity in module use. We apply modular networks both to image recognition and language modeling tasks, where we achieve superior performance compared to several baselines. Introspection reveals that modules specialize in interpretable contexts.
- [[Atachments]]
	- [kirsch_2018_modular_networks.pdf](zotero://select/library/items/Y5XULWAQ) {{zotero-linked-file "Kirsch/kirsch_2018_modular_networks.pdf"}}
	- [arXiv.org Snapshot](https://arxiv.org/abs/1811.05249) {{zotero-imported-file 9PHYCZI8, "1811.html"}}