links:: [Local library](zotero://select/library/items/QJ27H3MD), [Web library](https://www.zotero.org/users/8224007/items/QJ27H3MD)
authors:: [[Benoit Jacob]], [[Skirmantas Kligys]], [[Bo Chen]], [[Menglong Zhu]], [[Matthew Tang]], [[Andrew Howard]], [[Hartwig Adam]], [[Dmitry Kalenichenko]]
tags:: [[Computer Science - Machine Learning]], [[Statistics - Machine Learning]]
date:: [[Dec 15th, 2017]]
item-type:: [[preprint]]
title:: @Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference

- [[Abstract]]
	- The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.
- [[Atachments]]
	- [jacob_2017_quantization and training of neural networks for efficient.pdf](zotero://select/library/items/Z68TYKBW) {{zotero-linked-file "Jacob/jacob_2017_quantization and training of neural networks for efficient.pdf"}}